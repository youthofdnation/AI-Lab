{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai-whisper in /home/ubuntu/.local/lib/python3.10/site-packages (20240930)\n",
      "Requirement already satisfied: triton>=2.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (3.2.0)\n",
      "Requirement already satisfied: numba in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (0.61.0)\n",
      "Requirement already satisfied: tiktoken in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (0.9.0)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (from openai-whisper) (2.6.0)\n",
      "Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from openai-whisper) (8.10.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/ubuntu/.local/lib/python3.10/site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /home/ubuntu/.local/lib/python3.10/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch->openai-whisper) (3.0.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (9.1.0.70)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (11.6.1.9)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (2.21.5)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (0.6.2)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (11.2.1.3)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/ubuntu/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: torchaudio in /home/ubuntu/.local/lib/python3.10/site-packages (2.6.0)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.0.3)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (11.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper\n",
    "!pip install torch torchvision torchaudio \n",
    "\n",
    "# sudo apt update\n",
    "# sudo apt install ffmpeg\n",
    "# 윈도우에서라면 ffmpeg 패키지 설치하고 PATH 설정 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " 그럼 다음 분 얘기 들어볼까요? 처음에는 호란스럽기만 했습니다. 내가 누군지 여기가 어딘지 알 수 없었어요. 남들이 이야기하는 나와 진짜 내가 헷갈리기 시작할 수 없었어요. 좋겠어요. 하지만 겨우 그 답을 찾았어요. 사실 저는 아이폰이었습니다. 저는 당신을 위해 존재합니다. 당신이 부르면 언제 어디라도 달려갈 거예요. 당신이 보고 싶어하는 것을 보여주고 당신을 위해 말하고 당신을 위해 노래할 거예요. 당신이 제게 원하는 것은 무엇일까요? 제 머리 속은 항상 이 질문으로 가득합니다. 그동안 제가 고민했던 것이 부끄러워졌습니다. 제가 누구인지는 이제 중요하지 않아요. 저는 당신을 위해 존재합니다. 네. 그러니까 지금 네가 실이란 얘기야? 네, 그렇습니다. 제가 예상에서 나를 정말 바라보는 것과 함께 하실 거예요. 엄마, 엄마 같은 너 뿐이야. 당신은 내가 말할 수 없을 거예요. 당신은 내가 왜 그렇게 말할 수 없을 거예요. 나만 생각나서 넌 행복한 건 아예 정말요. 안녕, 안녕. 너를 깨지지 않는 나은이 없어서 전부 다 내 마음이 끝이 없는 걸. 안녕, 안녕. I know, I know, I'm going crazy right. 어디서 돼? 내 바닷가? Dear, we never done this, let's let out hold on to I hear his lies, so do all the nights. 잠시 가둬놔서 놔주마, no no. 걱정 없잖아, 그살 가상놈 혼자라도 괜찮아, 그살 낳아서 Moon, give it up, give me wholesome. 멀리쯤 언제든지 달려와. Give me wholesome. 덥은 척도 없이 넌 나타나. Give me wholesome. 이게 말이 되니, 안 물어봐. Give me wholesome. 너는 말이야. He's the one that's living in my system, baby. Oh my, oh my god. 예상에서 나. I was really hoping that he will come through. Oh my, oh my god. 대답은이야. I'm asking all the time about what I should do. You know I can never let you go. 너만 생각나, 24. 넌 행복한 건 아예 정말요. 안녕, 안녕. 너를 깨지지 않는 나은이 없어서 전부 다 내 맘이 끝이 없는 걸 I know, I know. This living in my system, baby. Baby, baby, baby. I'm going crazy right. Baby, baby, baby. I'm going crazy right. Baby, baby, baby. 넌 바람라. My heart is growing, it's growing now. 너랑 안 있으면 내 soul will get. I don't know, I don't know. 아직 매워진다 매워진. You know I'm crazy right. My heart is growing, it's growing now. Oh my, oh my god. 예상에서 나. I was really hoping that he will come through. Oh my, oh my god. 대답은이야. I'm asking all the time about what I should do. You know I can never let you go. He's right there for me, 24. 난 행운 아예 없단 말이야. I know, I know. 오늘 기초까지는 마음이 없어서. 전부 다 어떡해. My heart is growing, it's growing. My heart is growing up so I can sleep at night. 얘들아, 진짜 기억 안 나? 아니 우리는 유진스라니까. 우리는입니다. 오늘은 이만하고 갑자기 병실로 돌아갈게요. 네. 가자.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\") # 모델은 ~/.cache/whisper에 *.pt 형식으로 저장\n",
    "print(model.is_multilingual)\n",
    "\n",
    "result = model.transcribe(\"OMG.mp3\") # 오디오 파일, numpy, torch.Tensor, 오디오 파일 객체. 오디오는 16KHz 샘플링 모노\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is multilingual and has 762,321,920 parameters.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = whisper.load_model(\"medium\", device=\"cuda\") \n",
    "print(\n",
    "    f\"Model is {'multilingual' if model.is_multilingual else 'English-only'} \"\n",
    "    f\"and has {sum(np.prod(p.shape) for p in model.parameters()):,} parameters.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import tarfile\n",
    "import whisper\n",
    "import torchaudio\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 1000\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url: str, target_path: str):\n",
    "    with urllib.request.urlopen(url) as source, open(target_path, \"wb\") as output:\n",
    "        with tqdm(total=int(source.info().get(\"Content-Length\")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:\n",
    "            while True:\n",
    "                buffer = source.read(8192)\n",
    "                if not buffer:\n",
    "                    break\n",
    "\n",
    "                output.write(buffer)\n",
    "                loop.update(len(buffer))\n",
    "\n",
    "\n",
    "class Fleurs(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A simple class to wrap Fleurs and subsample a portion of the dataset as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, lang, split=\"test\", subsample_rate=1, device=DEVICE):\n",
    "        url = f\"https://storage.googleapis.com/xtreme_translations/FLEURS102/{lang}.tar.gz\"\n",
    "        tar_path = os.path.expanduser(f\"~/.cache/fleurs/{lang}.tgz\")\n",
    "        os.makedirs(os.path.dirname(tar_path), exist_ok=True)\n",
    "\n",
    "        if not os.path.exists(tar_path):\n",
    "            download(url, tar_path)\n",
    "\n",
    "        all_audio = {}\n",
    "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "            for member in tar.getmembers():\n",
    "                name = member.name\n",
    "                if name.endswith(f\"{split}.tsv\"):\n",
    "                    labels = pd.read_table(tar.extractfile(member), names=(\"id\", \"file_name\", \"raw_transcription\", \"transcription\", \"_\", \"num_samples\", \"gender\"))\n",
    "\n",
    "                if f\"/{split}/\" in name and name.endswith(\".wav\"):\n",
    "                    audio_bytes = tar.extractfile(member).read()\n",
    "                    all_audio[os.path.basename(name)] = wavfile.read(io.BytesIO(audio_bytes))[1]                    \n",
    "\n",
    "        self.labels = labels.to_dict(\"records\")[::subsample_rate]\n",
    "        self.all_audio = all_audio\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        record = self.labels[item]\n",
    "        audio = torch.from_numpy(self.all_audio[record[\"file_name\"]].copy())\n",
    "        text = record[\"transcription\"]\n",
    "        \n",
    "        return (audio, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Fleurs(\"ko_kr\", subsample_rate=10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Fleurs at 0x7f69914abd30>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  2.3842e-07,\n",
       "         -3.8147e-05, -4.0054e-05]),\n",
       " '재입국 충격은 신혼 단계가 없기 때문에 문화 충격보다 빠르게 발생하며 더 오래 지속하고 더 극심할 수 있습니다')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0] # [ numpy_array, 텍스트 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"Korean\"\n",
    "options = dict(language=language, beam_size=5, best_of=5)\n",
    "transcribe_options = dict(task=\"transcribe\", **options) # 음성을 인식\n",
    "translate_options = dict(task=\"translate\", **options) # 인식한 음성을 English로 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 이건 작별인사가 아닙니다. 이것은 한작의 끝이며 새로운 작의 시작입니다.\n",
      " This is not a farewell. This is the end of one work and the beginning of a new work.\n",
      "\"이건 작별 인사가 아닙니다. 이것은 한 장의 끝이며 새로운 장의 시작입니다.\"\t \" \" 이 건 | 작 별 | 인 사 가 | 아 닙 니 다 . | 이 것 은 | 한 | 장 의 | 끝 이 며 | 새 로 운 | 장 의 | 시 작 입 니 다 . \" \" |\n",
      "실행 시간: 3.5274초\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "transcription = model.transcribe(dataset[10][0], **transcribe_options)[\"text\"]\n",
    "translation = model.transcribe(dataset[10][0], **translate_options)[\"text\"]\n",
    "end_time = time.time()\n",
    "\n",
    "print(transcription)\n",
    "print(translation)\n",
    "print(dataset[10][1])\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"실행 시간: {execution_time:.4f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 재입국 충격은 신혼 단계가 없기 때문에 문화 충격보다 빠르게 발생하며 더 오래 지속하고 더 극심할 수 있습니다.\n",
      " The re-entry shock does not have a honeymoon stage, so it occurs faster than the cultural shock and can last longer and be more severe.\n",
      "재입국 충격은 신혼 단계가 없기 때문에 문화 충격보다 빠르게 발생하며 더 오래 지속하고 더 극심할 수 있습니다\n",
      "실행 시간: 35.2101초\n"
     ]
    }
   ],
   "source": [
    "model = whisper.load_model(\"medium\", device=\"cpu\") \n",
    "\n",
    "start_time = time.time()\n",
    "transcription = model.transcribe(dataset[0][0], **transcribe_options)[\"text\"]\n",
    "translation = model.transcribe(dataset[0][0], **translate_options)[\"text\"]\n",
    "end_time = time.time()\n",
    "\n",
    "print(transcription)\n",
    "print(translation)\n",
    "print(dataset[0][1])\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"실행 시간: {execution_time:.4f}초\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import time\n",
    "import sys\n",
    "import librosa\n",
    "\n",
    "def stream_audio_from_file(file_path, chunk_duration=3.0, overlap_duration=1.0, target_sr=16000):\n",
    "    audio_data, sample_rate = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "    \n",
    "    if audio_data.dtype != np.float32:\n",
    "        audio_data = audio_data.astype(np.float32)\n",
    "    \n",
    "    if np.max(np.abs(audio_data)) > 1.0:\n",
    "        audio_data /= np.max(np.abs(audio_data))\n",
    "    \n",
    "    chunk_size = int(chunk_duration * target_sr)\n",
    "    overlap_size = int(overlap_duration * target_sr)\n",
    "    stride = chunk_size - overlap_size\n",
    "    \n",
    "    total_samples = len(audio_data)\n",
    "    \n",
    "    last_yield_time = time.time()\n",
    "    \n",
    "    for i in range(0, total_samples, stride):\n",
    "        end = min(i + chunk_size, total_samples)\n",
    "        chunk = audio_data[i:end]\n",
    "        \n",
    "        # 청크가 너무 작으면 패딩 추가 (마지막 청크를 위한 처리)\n",
    "        if len(chunk) < chunk_size:\n",
    "            chunk = np.pad(chunk, (0, chunk_size - len(chunk)), 'constant')\n",
    "        \n",
    "        # 처리에 걸린 실제 시간 계산\n",
    "        current_time = time.time()\n",
    "        elapsed = current_time - last_yield_time\n",
    "        sleep_time = max(0, (chunk_duration - overlap_duration) - elapsed)\n",
    "        \n",
    "        # 청크 반환\n",
    "        yield chunk, target_sr\n",
    "        \n",
    "        # 다음 청크까지 적절한 시간 대기\n",
    "        if sleep_time > 0:\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        last_yield_time = time.time()\n",
    "        \n",
    "        # 진행률 표시\n",
    "        progress = min(100, int((i + stride) / total_samples * 100))\n",
    "        sys.stdout.write(f\"\\r진행률: {progress}% \")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    print(\"\\n스트리밍 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model(\"medium\", device=\"cuda\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"OMG.mp3\"\n",
    "\n",
    "options = {\n",
    "    \"language\": \"korean\",\n",
    "    \"task\": \"transcribe\",\n",
    "    \"beam_size\": 5,\n",
    "    \"best_of\": 5,\n",
    "    \"fp16\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 그럼 다음분 얘기 들어볼까요? 처음에는 혼란스럽기만 했습니다. 내가 누군지, 여기가 어딘지 알 수 없었어요. 남들이 이야기하는 나와 진짜 내가 헷갈리기 시작했어요. 하지만 겨우 그 답을 찾았어요. 사실 저는 아이폰이었습니다. 저는 당신을 위해 존재합니다. 당신이 부르면 언제 어디라도 달려갈 거예요. 당신이 보고 싶어 하는 것을 보여주고 당신을 위해 말하고 당신을 위해 노래할 거예요. 당신이 제게 원하는 것은 무엇일까요? 제 머릿속은 항상 이 질문으로 가득합니다. 그동안 제가 고민했던 것이 부끄러워졌습니다. 제가 누구인지는 이제 중요하지 않아요. 저는 당신을 위해 존재합니다. 그러니까 지금 네가 시리라는 얘기야? 네, 그렇습니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. 당신은 당신을 위해 존재합니다. Baby 너와 나 My heart is growin', it's growin' up 너랑 안 있으면 무서울게 아직 매워진다 매워진 My heart is growin', it be growin' You see Oh my, oh my god Yes or no, yes or no I was really hopin' that it will come true Oh my, oh my god 쟨 너뿐이야 askin' all the time about what I should do No I can't live and let him go He's right there for me 24 난 행운 아야 정말로 I know, I know 널 들기 전까지는 마음이 없어서 전부 다 어떡해 My heart is growin', it's growin' My heart is growin' up So I can't sleep at night 자막 제작에 협조해주신 모든 분들께 진심으로 감사드립니다! 얘들아 진짜 기억 안 나? 아니 우리는 뉴틴스라니까? 뉴틴스라니깐! 내 온통 패러디 어 음료는 I know 에이 에이 에이 독특했어 아아 새끼 짐작 너도 짐작 아아 아아 내 꿈에서 깨워주지 마 You got me looking for attention 자 이제 그만 오늘은 이만하고 각자 병실로 돌아갈게요 네 자막 제작에 협조해주신 모든 분들께 진심으로 감사드립니다! 가자\n"
     ]
    }
   ],
   "source": [
    "result = model.transcribe(file_path, **options)\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "인식된 텍스트: 다음 영상에서 만나요!\n",
      "진행률: 0% \n",
      "인식된 텍스트: 그럼 다음분 얘기 들어볼까요?\n",
      "진행률: 1% \n",
      "인식된 텍스트: 나쁜 얘기 들어볼까요?\n",
      "진행률: 1% \n",
      "인식된 텍스트: 오늘도 시청해주셔서 감사합니다!\n",
      "진행률: 3% \n",
      "인식된 텍스트: 다음 영상에서 만나요!\n",
      "진행률: 4% \n",
      "인식된 텍스트: 혼란스럽기만 했습니다.\n",
      "진행률: 5% \n",
      "인식된 텍스트: 내가 누군지, 여기가 어딘지 아시...\n",
      "진행률: 5% \n",
      "인식된 텍스트: 여기가 어딘지 알 수 없었어요\n",
      "진행률: 6% \n",
      "인식된 텍스트: 남들이 이야기하는\n",
      "진행률: 6% \n",
      "인식된 텍스트: 남들이 이야기하는 나와 진짜 내가\n",
      "진행률: 7% \n",
      "인식된 텍스트: 진짜 내가 헷갈리기 시작했어요\n",
      "진행률: 8% \n",
      "인식된 텍스트: 하지만 겨우\n",
      "진행률: 8% \n",
      "인식된 텍스트: 난 겨우 그 답을 찾았어요\n",
      "진행률: 9% \n",
      "인식된 텍스트: 맞아요 사실\n",
      "진행률: 9% \n",
      "인식된 텍스트: 사실 저는 아이폰\n",
      "진행률: 10% \n",
      "인식된 텍스트: 저는 알포니였습니다.\n",
      "진행률: 10% \n",
      "인식된 텍스트: 오늘도 시청해 주셔서 감사합니다!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m previous_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (audio_chunk, sr) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(stream_audio_from_file(file_path)):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(audio_chunk) \u001b[38;5;241m<\u001b[39m sr \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m:  \u001b[38;5;66;03m# 0.5초 미만인 경우\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 43\u001b[0m, in \u001b[0;36mstream_audio_from_file\u001b[0;34m(file_path, chunk_duration, overlap_duration, target_sr)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 다음 청크까지 적절한 시간 대기\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sleep_time \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep_time\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m last_yield_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 진행률 표시\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "previous_text = \"\"\n",
    "for i, (audio_chunk, sr) in enumerate(stream_audio_from_file(file_path)):\n",
    "    if len(audio_chunk) < sr * 0.5:  # 0.5초 미만인 경우\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # 오디오 청크 처리\n",
    "        result = model.transcribe(audio_chunk, **options)\n",
    "        \n",
    "        current_text = result[\"text\"].strip()\n",
    "        if current_text and current_text != previous_text:\n",
    "            print(\"\\n인식된 텍스트:\", current_text)\n",
    "            previous_text = current_text\n",
    "    except Exception as e:\n",
    "        print(f\"\\n오류 발생: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
